---
title: "Data Science II HW 2"
author: "Maya Krishnamoorthy"
date: "2025-03-13"
output: 
  pdf_document:
    toc: true
    number_sections: true
fontsize: 11pt
header-includes:
  - \usepackage{booktabs}  # For better tables
  - \usepackage{graphicx}
---

```{r message=FALSE, echo=FALSE}
library(caret)
library(tidymodels)
library(splines)
library(mgcv)
library(pdp)
library(earth)
library(tidyverse)
library(ggplot2)
```

Read in data and set up train/test split.

```{r}
college = 
  read_csv("College.csv") %>% 
  janitor::clean_names()

set.seed(2025)

data_split = initial_split(college, prop = 0.8)

training_data = training(data_split) %>% select(-college)
testing_data = testing(data_split) %>% select(-college)
```


## Part a)

**Fit smoothing spline models to predict out-of-state tuition (Outstate) using the percentage of alumni who donate (perc.alumni) as the only predictor, across a range of degrees of freedom. Plot the fitted curve for each degree of freedom. Describe the patterns you observe as the degrees of freedom change. Choose an appropriate degree of freedom for the model and plot this optimal fit. Explain the criteria you used to select the degree of freedom.**

```{r}
p = 
  ggplot(data = training_data, aes(x = perc_alumni, y = outstate)) +
  geom_point(color = rgb(.2, .4, .2, .5))

perc_alumni.grid = seq(from = -10, to = 110, by = 1) # the range of perc_alumni is [0, 100]

fit.ss = smooth.spline(training_data$perc_alumni, training_data$outstate)

pred.ss = predict(fit.ss, x = perc_alumni.grid)
pred.ss.df = data.frame(pred = pred.ss$y, perc_alum = perc_alumni.grid)

p + geom_line(aes(x = perc_alum, y = pred), data = pred.ss.df,
color = rgb(.8, .1, .1, 1)) + theme_bw()

cv_df = fit.ss$df # generalized CV method uses df = 3.88007
```


Testing different df values:

```{r}
df_values = 2:15
spline_list = list()

test_mse = numeric(length(df_values))

# Loop through df values and store predictions
for (i in seq_along(df_values)) {
  df = df_values[i]

  fit = smooth.spline(training_data$perc_alumni, training_data$outstate, df = df)

  pred_train = predict(fit, x = perc_alumni.grid)
  pred_test = predict(fit, x = testing_data$perc_alumni)
  
  spline_list[[df]] = tibble(
    perc_alum = perc_alumni.grid,
    pred = pred_train$y,
    df = factor(df)  # convert df to a factor for faceting
  )
  
  test_mse[i] = sqrt(mean((testing_data$outstate - pred_test$y)^2))
}

best_df = df_values[which.min(test_mse)]

spline_predictions = bind_rows(spline_list)

ggplot(spline_predictions, aes(x = perc_alum, y = pred)) +
  geom_line(color = "red") +
  facet_wrap(~df, ncol = 3) +  # Arrange in 3 columns
  labs(title = "Smoothing Spline Fits with Different Degrees of Freedom",
       x = "% Alumni who donate",
       y = "Out of state tuition",
       caption = "Each panel represents a different degree of freedom") +
  theme_bw()

mse_df = tibble(df = df_values, MSE = test_mse)

ggplot(mse_df, aes(x = df, y = MSE)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  labs(title = "Test MSE vs. Degrees of Freedom",
       x = "Degrees of Freedom",
       y = "Test RMSE") +
  theme_minimal()
```

These plots show the spline curves for different degrees of freedom between 3 and 15. For lower df (2-5), the splines are very smooth and almsot completely linear, which may be a sign of not capturing the data entirely. For higher df (>10), the splines may be overfitting the data even though they are less linear. The df resulting from the generalized CV method is `r fit.ss$df` using the generalized CV method. The best df in the manual range is 2.


## Part b)

```{r}
# matrix of predictors
x = model.matrix(outstate ~ ., training_data)[, -1]
# vector of response
y = training_data$outstate

test_x = model.matrix(outstate ~ ., testing_data)[, -1]
test_y = testing_data$outstate
```

**Fitting a model:**

```{r}
set.seed(2025)

ctrl1 = trainControl(method = "cv", number = 10)

# create grid of all possible pairs that can take degree and nprune values
mars_grid = expand.grid(degree = 1:3, # number of possible product hinge functions in 1 term
                         nprune = 2:18) # Upper bound of number of terms in model

mars.fit = train(x, y, # training dataset
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1) # 10-fold CV

ggplot(mars.fit)
```


```{r}
mars.fit$bestTune
coef(mars.fit$finalModel)

# Calculate MSE
mars.pred = predict(mars.fit, newdata = test_x)
mse.mars = mean((mars.pred - test_y)^2)
```
The final model uses 1 product degree, meaning that there should be no hinge function, and 13 total terms are selected. The most important predictors are `perc_alumni`, `grad_rate`, and `ph_d`. The test error (MSE) is 3497772.

**Partial dependency plots:**

```{r}
p1 = pdp::partial(mars.fit, pred.var = c("grad_rate"), grid.resolution = 10) |> autoplot()
p2 = pdp::partial(mars.fit, pred.var = c("grad_rate", "room_board"), grid.resolution = 10) |>
pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 20, x = -60))
gridExtra::grid.arrange(p1, p2, ncol = 2)
```
The partial dependency plot on the left represents the relationship between `grad_rate` and `outstate`, whereas the one on the right represents the 2D relationship between `room_board` and `grad_rate` with `outstate`.

## Part c) 

**Construct a generalized additive model (GAM) to predict the response variable. For the nonlinear terms included in your model, generate plots to visualize these relationships and discuss your observations. Report the test error.**

```{r}
set.seed(2025)

gam.fit = train(x, y,
                method = "gam",
                tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                trControl = ctrl1
)

gam.fit$finalModel
summary(gam.fit)
```

**Generate plots:**

```{r}
par(mar = c(2,2,2,2), mfrow = c(4,4))
plot(gam.fit$finalModel)
```
**Predict the test error:**
```{r}
gam.pred = predict(gam.fit, newdata = test_x)
gam.mse = mean((gam.pred - test_y)^2)
```

The GAM model includes all predictors. After graphing the predictor-outcome relationships (left-right, top-bottom), we can say almost surely that `books`, `enroll`, `accept`, `f_undergrad`, `expend`, along with a few others, are non-linear. The MSE of the GAM model is `r gam.mse`.

## Part d)

**In this dataset, would you favor a MARS model over a linear model for predicting out-of-state tuition? If so, why? More broadly, in general applications, do you consider a MARS model to be superior to a linear model? Please share your reasoning.**

Fit a linear model:

```{r}
set.seed(2025)

lm.fit = train(x, y, method = "lm", trControl = ctrl1)
summary(lm.fit)

lm.pred = predict(lm.fit, newdata = test_x)
lm.mse = mean((lm.pred - test_y)^2);lm.mse
```

Resampling:
```{r}
resamp = resamples(list(mars = mars.fit,
                        lm = lm.fit))

summary(resamp)
bwplot(resamp, metric = "RMSE")
```
The MARS model has a far lower RMSE than the linear model, so it would be better for predicting out-of-state tuition. MARS models can be preferable over linear models in the situation when the relationships between the predictor(s) and the outcome are not linear. If there is interaction between predictors, MARS may perform better than linear models. However, linear models are more easily interpretable for simple data, so in that situation, linear models may be preferable over MARS.
