P8106 HW 2
================
Maya Krishnamoorthy
2025-03-09

Read in data and set up train/test split.

``` r
college = 
  read_csv("College.csv") %>% 
  janitor::clean_names()
```

    ## Rows: 565 Columns: 18
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr  (1): College
    ## dbl (17): Apps, Accept, Enroll, Top10perc, Top25perc, F.Undergrad, P.Undergr...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
set.seed(2025)

data_split = initial_split(college, prop = 0.8)

training_data = training(data_split) %>% select(-college)
testing_data = testing(data_split) %>% select(-college)
```

## Part a)

**Fit smoothing spline models to predict out-of-state tuition (Outstate)
using the percentage of alumni who donate (perc.alumni) as the only
predictor, across a range of degrees of freedom. Plot the fitted curve
for each degree of freedom. Describe the patterns you observe as the
degrees of freedom change. Choose an appropriate degree of freedom for
the model and plot this optimal fit. Explain the criteria you used to
select the degree of freedom.**

``` r
p = 
  ggplot(data = training_data, aes(x = perc_alumni, y = outstate)) +
  geom_point(color = rgb(.2, .4, .2, .5))

perc_alumni.grid = seq(from = -10, to = 110, by = 1) # the range of perc_alumni is [0, 100]

fit.ss = smooth.spline(training_data$perc_alumni, training_data$outstate)

pred.ss = predict(fit.ss, x = perc_alumni.grid)
pred.ss.df = data.frame(pred = pred.ss$y, perc_alum = perc_alumni.grid)

p + geom_line(aes(x = perc_alum, y = pred), data = pred.ss.df,
color = rgb(.8, .1, .1, 1)) + theme_bw()
```

![](p8106_hw2_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
cv_df = fit.ss$df # generalized CV method uses df = 3.88007
```

Testing different df values:

``` r
# Define df range
df_values <- 2:15

# Create an empty list to store data frames
spline_list <- list()

test_mse <- numeric(length(df_values))

# Loop through df values and store predictions
for (i in seq_along(df_values)) {
  df <- df_values[i]
  
  # Fit smoothing spline
  fit <- smooth.spline(training_data$perc_alumni, training_data$outstate, df = df)
  
  # Get predictions
  pred_train <- predict(fit, x = perc_alumni.grid)
  
  pred_test <- predict(fit, x = testing_data$perc_alumni)
  
  # Store results in a data frame
  spline_list[[df]] <- tibble(
    perc_alum = perc_alumni.grid,
    pred = pred_train$y,
    df = factor(df)  # Convert df to a factor for faceting
  )
  
  # Compute test MSE
  test_mse[i] <- sqrt(mean((testing_data$outstate - pred_test$y)^2))
}

best_df <- df_values[which.min(test_mse)]

# Combine all predictions into one data frame
spline_predictions <- bind_rows(spline_list)

# Plot with facet_wrap()
ggplot(spline_predictions, aes(x = perc_alum, y = pred)) +
  geom_line(color = "red") +
  facet_wrap(~df, ncol = 3) +  # Arrange in 3 columns
  labs(title = "Smoothing Spline Fits with Different Degrees of Freedom",
       x = "% Alumni who donate",
       y = "Out of state tuition",
       caption = "Each panel represents a different degree of freedom") +
  theme_bw()
```

![](p8106_hw2_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

``` r
# Plot MSE vs. Degrees of Freedom
mse_df <- tibble(df = df_values, MSE = test_mse)

ggplot(mse_df, aes(x = df, y = MSE)) +
  geom_point(color = "blue") +
  geom_line(color = "blue") +
  labs(title = "Test MSE vs. Degrees of Freedom",
       x = "Degrees of Freedom",
       y = "Test RMSE") +
  theme_minimal()
```

![](p8106_hw2_files/figure-gfm/unnamed-chunk-4-2.png)<!-- -->

These plots show the spline curves for different degrees of freedom
between 3 and 15. For lower df (2-5), the splines are very smooth and
almsot completely linear, which may be a sign of not capturing the data
entirely. For higher df (\>10), the splines may be overfitting the data
even though they are less linear. The df resulting from the generalized
CV method is 3.8800698 using the generalized CV method. The best df in
the manual range is 2.

## Part b) MARS MODEL

``` r
# matrix of predictors
x <- model.matrix(outstate ~ ., training_data)[, -1]
# vector of response
y <- training_data$outstate

test_x <- model.matrix(outstate ~ ., testing_data)[, -1]
test_y <- testing_data$outstate
```

**Fitting a model:**

``` r
set.seed(2025)

ctrl1 <- trainControl(method = "cv", number = 10)

# create grid of all possible pairs that can take degree and nprune values
mars_grid <- expand.grid(degree = 1:3, # number of possible product hinge functions in 1 term
                         nprune = 2:18) # Upper bound of number of terms in model

mars.fit <- train(x, y, # training dataset
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1) # 10-fold CV

ggplot(mars.fit)
```

![](p8106_hw2_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

``` r
mars.fit$bestTune
```

    ##    nprune degree
    ## 12     13      1

``` r
coef(mars.fit$finalModel)
```

    ##         (Intercept)     h(14980-expend)     h(80-grad_rate)  h(4328-room_board) 
    ##       17334.2899355          -0.6464064         -28.9745892          -1.0089007 
    ## h(1405-f_undergrad)   h(15-perc_alumni)        h(apps-6548)        h(6548-apps) 
    ##          -1.3839040        -146.6145853           0.3609300          -0.7307644 
    ##       h(enroll-913)       h(913-enroll)    h(1344-personal)          h(ph_d-81) 
    ##          -2.0498812           5.6849986           0.9274978          64.9997133 
    ##      h(1611-accept) 
    ##          -0.9780810

``` r
# Calculate MSE.
mars.pred = predict(mars.fit, newdata = test_x)
mse.mars = mean((mars.pred - test_y)^2)
```

The final model uses 1 product degree, meaning that there should be no
hinge function, and 13 total terms are selected. The most important
predictors are `perc_alumni`, `grad_rate`, and `ph_d`. The test error
(MSE) is 3497772.

**Partial dependency plots:**

``` r
p1 <- pdp::partial(mars.fit, pred.var = c("grad_rate"), grid.resolution = 10) |> autoplot()
p2 <- pdp::partial(mars.fit, pred.var = c("grad_rate", "room_board"), grid.resolution = 10) |>
pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 20, x = -60))
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

![](p8106_hw2_files/figure-gfm/unnamed-chunk-8-1.png)<!-- --> The
partial dependency plot on the left represents the relationship between
`grad_rate` and `outstate`, whereas the one on the right represents the
2D relationship between `room_board` and `grad_rate` with `outstate`.
